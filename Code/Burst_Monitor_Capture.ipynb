{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "904bcee3",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4fe483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "\n",
    "import datetime\n",
    "from datetime import date, time, timedelta, datetime\n",
    "\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import pandas_market_calendars as mcal\n",
    "\n",
    "import importlib\n",
    "import config\n",
    "importlib.reload(config)\n",
    "\n",
    "# import time\n",
    "from time import sleep\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.models import ColumnDataSource, CustomJSTickFormatter, DatetimeTickFormatter, DatetimeTicker, HoverTool,CustomJSHover, LinearAxis, Range1d\n",
    "from bokeh.io import output_notebook\n",
    "from bokeh.io import output_file, save, show, reset_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f8e9a6",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9abd69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date from a filename where the date is in the format YYYYMMDD convert to datetime\n",
    "def extract_date_from_filename_datetime(filename):\n",
    "    date = re.search(r'\\d{8}', filename)\n",
    "    if date:\n",
    "        return datetime.strptime(date.group(0), '%Y%m%d')\n",
    "    return None\n",
    "\n",
    "# Convert an integer representing a time in the format HHMMSS to a time object\n",
    "def int_to_time(time_int):\n",
    "    time_str = str(time_int).zfill(6)\n",
    "    hour = int(time_str[:2])\n",
    "    minute = int(time_str[2:4])\n",
    "    second = int(time_str[4:])\n",
    "    return time(hour=hour, minute=minute, second=second)\n",
    "\n",
    "# Convert and individual HFT1.csv file to a DataFrame using a threshold on the 'NumQuotes' column\n",
    "def make_df_from_high_quote_count(file_path, threshold):\n",
    "    temp_date = extract_date_from_filename_datetime(file_path)    \n",
    "    # Read the CSV file\n",
    "    df = pl.read_csv(file_path, has_header=False)    \n",
    "    # Rename the columns\n",
    "    df = df.rename({\n",
    "        'column_1': 'Symbol',\n",
    "        'column_2': 'T1',\n",
    "        'column_3': 'Event_Date_Time',\n",
    "        'column_4': 'NumQuotes',\n",
    "        'column_5': 'BBO',\n",
    "        'column_6': 'NumTrades',\n",
    "        'column_7': 'Dollars',\n",
    "        'column_8': 'T3'\n",
    "    })\n",
    "    \n",
    "    # # Apply transformations\n",
    "    # df = df.with_columns(pl.col('Event_Date_Time').map_elements(int_to_time, return_dtype=pl.Time).alias('Event_Date_Time'))\n",
    "    # df = df.with_columns(pl.col('Event_Date_Time').map_elements(lambda x: datetime.combine(temp_date, x), return_dtype=pl.Datetime).alias('Event_Date_Time')) \n",
    "\n",
    "        # Apply transformations\n",
    "    df = df.with_columns(pl.col('Event_Date_Time').map_elements(int_to_time, return_dtype=pl.Time).alias('Event_Date_Time'))\n",
    "    df = df.with_columns(pl.col('Event_Date_Time').map_elements(lambda x: datetime.combine(temp_date, x), return_dtype=pl.Datetime).alias('Event_Date_Time'))    \n",
    "\n",
    "    # Drop rows with NaN in 'Dollars' column\n",
    "    df = df.drop_nulls(subset=['Dollars'])   \n",
    "    # Filter rows where 'NumQuotes' is greater than the threshold\n",
    "    df = df.filter(pl.col('NumQuotes') > threshold)  \n",
    "    # Calculate 'Trade_Size'\n",
    "    df = df.with_columns((pl.col('Dollars') / pl.col('NumTrades')).alias('Trade_Size'))   \n",
    "    # Sort by 'Event_Date_Time'\n",
    "    df = df.sort('Event_Date_Time')\n",
    "    return df\n",
    "\n",
    "def load_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        return pl.read_parquet(file_path)\n",
    "    return None\n",
    "\n",
    "def save_data(df, file_path):\n",
    "    df.write_parquet(file_path)\n",
    "\n",
    "# def get_price_data_for_symbol(client, symbol, start_date, end_date):\n",
    "#     # print(f\"Getting price data for symbol: {symbol} from {start_date} to {end_date}\")\n",
    "#     resp = client.get_price_history(\n",
    "#         symbol=symbol,\n",
    "#         period_type=client.PriceHistory.PeriodType.DAY,\n",
    "#         period=client.PriceHistory.Period.ONE_DAY,\n",
    "#         frequency_type=client.PriceHistory.FrequencyType.MINUTE,\n",
    "#         frequency=client.PriceHistory.Frequency.EVERY_FIVE_MINUTES,\n",
    "#         start_datetime=start_date,\n",
    "#         end_datetime=end_date,\n",
    "#         need_previous_close='True'\n",
    "#     )\n",
    "\n",
    "#     assert resp.status_code == httpx.codes.OK\n",
    "#     history = resp.json()\n",
    "#     candles = history.get('candles', [])\n",
    "\n",
    "#     # Convert `datetime` values and write back into the `candles` list of dictionaries\n",
    "#     for candle in candles:\n",
    "#         dt_object = datetime.fromtimestamp(candle['datetime'] / 1000)\n",
    "#         # candle['Event_Date_Time'] = dt_object.isoformat()\n",
    "#         candle['Event_Date_Time'] = dt_object\n",
    "#         candle['Symbol'] = symbol  # Add symbol to each candle\n",
    "\n",
    "#     # Create a polars DataFrame from the candles list\n",
    "#     price_data_df = pl.DataFrame(candles)\n",
    "#     # return price_data_df\n",
    "\n",
    "def update_burst_data_df(concatenated_df ,directory_path, threshold, start_date):\n",
    "    # Set path to store processed dataframe\n",
    "    file_path = os.path.join(directory_path, 'burst_data_processed.parquet')\n",
    "    #List all files in the directory\n",
    "    files = os.listdir(directory_path)\n",
    "    # Filter the list to include only files that end with '.csv' and contain '_hft1' in the filename\n",
    "    filtered_files = [f for f in files if f.endswith('.csv') and '_hft1' in f]\n",
    "    # Find files after the data_start_date but do not process today's file\n",
    "    # Get today's date\n",
    "    today_date = datetime.today().strftime('%Y%m%d')\n",
    "    selected_files = []\n",
    "    for f in filtered_files:\n",
    "        try:\n",
    "            file_date_str = f.split('_')[0]\n",
    "            file_date = datetime.strptime(file_date_str, '%Y%m%d')\n",
    "            if file_date_str >= start_date and file_date != today_date:\n",
    "                selected_files.append(f)\n",
    "        except ValueError:\n",
    "            # Skip files that do not match the expected date format\n",
    "            continue\n",
    "\n",
    "    # Loop through selected files in the directory\n",
    "    for filename in selected_files:\n",
    "        # Generate a dataframe from the CSV file using the make_df_from_high_quote_count function\n",
    "        df = make_df_from_high_quote_count(os.path.join(directory_path, filename), threshold)\n",
    "        if df.height > 0:\n",
    "            # Concatenate the generated dataframe with the existing concatenated dataframe\n",
    "            concatenated_df = pl.concat([concatenated_df, df])\n",
    "    \n",
    "    save_data(concatenated_df, file_path)\n",
    "    return concatenated_df\n",
    "\n",
    "# def update_price_data_df(all_price_data_df, directory_path, burst_dataframe, client, start_date, end_date):\n",
    "#     # Set path to store processed dataframe\n",
    "#     file_path = os.path.join(directory_path, 'price_data_processed.parquet')\n",
    "\n",
    "#     # Extract a list of unique symbols from the burst_data_df\n",
    "#     unique_symbols = burst_dataframe.select(pl.col(\"Symbol\").unique()).to_series().to_list()\n",
    "#     # Extract the unique symbols from all_price_data_df\n",
    "#     existing_symbols = all_price_data_df.select(pl.col(\"Symbol\").unique()).to_series().to_list()\n",
    "\n",
    "#         # Check if all_price_data_df is empty\n",
    "#     if all_price_data_df.is_empty():\n",
    "#         # If empty, fetch price data for all symbols from start_date to end_date\n",
    "#         for symbol in unique_symbols:\n",
    "#             symbol_price_data_df = get_price_data_for_symbol(client, symbol, start_date, end_date)\n",
    "#             if symbol_price_data_df.height != 0:\n",
    "#                 all_price_data_df = pl.concat([all_price_data_df, symbol_price_data_df])\n",
    "#     else:\n",
    "#        # For all the symbols that we already have price dat for check to see if we have the latest data for the symbol. If we do not have the latest data for the symbol then update the price data for the symbol\n",
    "#         max_dates_df = all_price_data_df.group_by(\"Symbol\").agg(pl.col(\"Event_Date_Time\").max().alias(\"Max_Event_Date_Time\"))\n",
    "#         update_target_df = max_dates_df.filter(pl.col(\"Max_Event_Date_Time\") < end_date)\n",
    "#         update_list = update_target_df.select(pl.col(\"Symbol\")).to_series().to_list()\n",
    "#         print(f'update_list length {len(update_list)}')\n",
    "#         # Add symbols that are not in the filtered DataFrame (i.e., new symbols)\n",
    "#         new_symbols = [symbol for symbol in unique_symbols if symbol not in existing_symbols]\n",
    "#         print(f'new update_symbols length {len(new_symbols)}')\n",
    "#         new_symbols = [symbol for symbol in unique_symbols if symbol not in existing_symbols]\n",
    "#         update_list.extend(new_symbols)\n",
    "#         print(f'total update_list length {len(update_list)}')\n",
    "#         i=len(update_list)\n",
    "#         #Process each unique symbol by checking if the symbol is already in the price data dataframe and if so updateing the price data for that symbol fro current end date of data to end_date\n",
    "#         for symbol in update_list:\n",
    "#             i=i-1\n",
    "#             print(f'Currentlly processing symbol {symbol} remaining symbols {i}')\n",
    "#             #Filter the price data dataframe for the symbol\n",
    "#             symbol_price_data_df = all_price_data_df.filter(pl.col(\"Symbol\") == symbol)\n",
    "#             #If the symbol is not in the price data dataframe then get the price data for the symbol from the current end date of the data to the end date\n",
    "#             if symbol_price_data_df.height == 0:\n",
    "#                 print(f\"Getting price data for new symbol: {symbol} from {start_date} to {end_date}\")\n",
    "#                 symbol_price_data_df = get_price_data_for_symbol(client, symbol, start_date, end_date)\n",
    "#             #If the symbol is in the price data dataframe then get the price data for the symbol from the current end date of the data to the end date\n",
    "#             else:\n",
    "#                 symbol_start_date = symbol_price_data_df['Event_Date_Time'].max()\n",
    "#                 print(f\"{symbol} Symbol start date: {symbol_start_date} type: {type(symbol_start_date)} end_date: {end_date} type: {type(end_date)} remaining symbols: {i}\")\n",
    "#                 # symbol_start_date = datetime.strptime(symbol_start_date, '%Y-%m-%dT%H:%M:%S')\n",
    "#                 # print(f\"Symbol start date (after): {symbol_start_date} type: {type(symbol_start_date)}\")\n",
    "                \n",
    "#                 # Check if symbol_start_date is less than end_date\n",
    "#                 if symbol_start_date < end_date:\n",
    "#                     print(f\"Getting price data for symbol: {symbol} from {symbol_start_date} to {end_date}\")\n",
    "#                     symbol_price_data_df = get_price_data_for_symbol(client, symbol, symbol_start_date, end_date)\n",
    "                \n",
    "#             #Concatenate the symbol price data to the all price data dataframe\n",
    "#             if symbol_price_data_df.height != 0:\n",
    "#                 all_price_data_df = pl.concat([all_price_data_df, symbol_price_data_df])\n",
    "\n",
    "#     save_data(all_price_data_df, file_path) \n",
    "#     return all_price_data_df\n",
    "\n",
    "def generate_alert_strings(full_dataframe,todays_dataframe):\n",
    "    # This function returns a message associated with the first occurrence of each symbol in the DataFrame.\n",
    "\n",
    "    # Get the unique symbols in the DataFrame\n",
    "    # symbols = todays_dataframe['Symbol'].unique()\n",
    "\n",
    "        # Group by Symbol and get the first Event_Date_Time for each symbol\n",
    "    first_appearance = todays_dataframe.group_by('Symbol').agg(pl.col('Event_Date_Time').min().alias('First_Appearance'))\n",
    "\n",
    "    # Sort by Event_Date_Time\n",
    "    sorted_symbols = first_appearance.sort('First_Appearance')['Symbol'].to_list()\n",
    "\n",
    "    print(sorted_symbols)\n",
    "    \n",
    "    # Initialize an empty list to store the alert strings\n",
    "    alert_strings = []\n",
    "    \n",
    "    # Iterate through each symbol in Todays DataFrame\n",
    "    for symbol in sorted_symbols:\n",
    "    # Filter the rows for the current symbol\n",
    "        todays_symbol_rows = todays_dataframe.filter(pl.col('Symbol') == symbol)\n",
    "        \n",
    "        # Ensure that there is at least one row for the current symbol\n",
    "        if todays_symbol_rows.height > 0:\n",
    "            num_quotes = todays_symbol_rows.select(pl.col('NumQuotes')).to_series().item(0)\n",
    "            num_trades = todays_symbol_rows.select(pl.col('NumTrades')).to_series().item(0)\n",
    "            dollars = todays_symbol_rows.select(pl.col('Dollars')).to_series().item(0)\n",
    "        else:\n",
    "            raise ValueError(f\"No rows found for symbol {symbol}\")\n",
    "        \n",
    "        # Get the event date and time from the first row for the current symbol\n",
    "        event_date_time = todays_symbol_rows.head(1).select(pl.col('Event_Date_Time')).item()\n",
    "\n",
    "        all_symbol_rows = full_dataframe.filter(pl.col('Symbol') == symbol)\n",
    "        \n",
    "        # Find the previous occurrence of the symbol in the full_dataframe\n",
    "        prev_event_date_time = None\n",
    "\n",
    "        if all_symbol_rows.height > 0:\n",
    "            prev_event_date_time = all_symbol_rows.tail(1).select(pl.col('Event_Date_Time')).to_series().item()\n",
    "        \n",
    "        # Calculate the interval between the current and previous occurrences\n",
    "        if prev_event_date_time is not None:\n",
    "            interval = (event_date_time - prev_event_date_time).days+1\n",
    "        else:\n",
    "            interval = None\n",
    "        \n",
    "        # Get the trade size from the first row of todays_symbol_rows\n",
    "        trade_size = todays_symbol_rows.head(1).select(pl.col('Dollars')).to_series().item()\n",
    "        # Count the number of appearances in all_symbol_rows\n",
    "        n_appearances = all_symbol_rows.select(pl.col('Dollars')).count().item() + 1\n",
    "        # Count the number of rows in all_symbol_rows where 'Dollars' is less than trade_size\n",
    "        trade_size_rank = n_appearances - all_symbol_rows.filter(pl.col('Dollars') < trade_size).select(pl.col('Dollars')).count().item()\n",
    "\n",
    "        \n",
    "        # Generate the alert string for the current symbol\n",
    "        if interval is not None:\n",
    "            alert_string = f\"HQC on ${symbol} with {num_quotes} quote changes resulting in {num_trades} trades  @ {event_date_time}. Last appeared on {prev_event_date_time}, {interval} days ago. Trade size is ${trade_size:,.2f} @ rank {trade_size_rank} for Size out of {n_appearances} occurrences.\"\n",
    "        else:\n",
    "            alert_string = f\"HQC on ${symbol} with {num_quotes} quote changes resulting in {num_trades} trades  @ {event_date_time}. This is the first appearance of symbol. Trade size is ${trade_size:,.2f} @ rank {trade_size_rank} for Size out of {n_appearances} occurrences.\"\n",
    "        \n",
    "        # Append the alert string to the list of alert strings\n",
    "        alert_strings.append(alert_string)\n",
    "    \n",
    "    # Return the list of alert strings\n",
    "    return alert_strings,sorted_symbols\n",
    "\n",
    "def split_message(message, max_length=140):\n",
    "    if len(message) <= max_length:\n",
    "        return [message]\n",
    "    \n",
    "    result = []\n",
    "    current_piece = ''\n",
    "    \n",
    "    for word in message.split():\n",
    "        if len(current_piece + word) + 1 > max_length:\n",
    "            result.append(current_piece.strip())\n",
    "            current_piece = ''\n",
    "        \n",
    "        current_piece += f'{word} '\n",
    "    \n",
    "    if current_piece:\n",
    "        result.append(current_piece.strip())\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fbac3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\vande\\Desktop\\Projects\\Ki2_Alerts\\Code\n",
      "Today's date string: 20250725\n",
      "Today's date string in year/month/day format: 2025/07/20250725\n",
      "Today's csv_path: ./Code/Data/20250725_hft1.csv\n",
      "Today's parquet_path: ./Processed_Data/2025/07/20250725_hft1.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "today_str = datetime.today().strftime('%Y%m%d')\n",
    "print(f\"Today's date string: {today_str}\")\n",
    "\n",
    "# make today-str year/month/day format\n",
    "today_str1 = datetime.today().strftime('%Y/%m/')\n",
    "print(f\"Today's date string in year/month/day format: {today_str1}{today_str}\")\n",
    "\n",
    "csv_path = f\"./Code/Data/{today_str}_hft1.csv\"\n",
    "print(f\"Today's csv_path: {csv_path}\")\n",
    "parquet_path = f\"./Processed_Data/{today_str1}{today_str}_hft1.parquet\"\n",
    "print(f\"Today's parquet_path: {parquet_path}\")\n",
    "last_size_path = f\"./Code/Data/{today_str}_hft1_lastsize.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d3bd8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INTC', 'PHIO', 'QQQ', 'IWM', 'QS', 'ASTS', 'QBTS', 'IOVA', 'MU', 'OPEN', 'FLG', 'IREN', 'ACHR', 'CMCSA', 'APA', 'SPY', 'NVDA', 'FAST', 'DHI', 'KMI', 'XLK', 'SOXL', 'ATAI', 'AAPL', 'XLI', 'TQQQ', 'PTC', 'IVZ', 'LBRT', 'YINN', 'IGV', 'LIDR', 'NEM', 'MBLY', 'POR', 'BBWI', 'OKLO', 'USO', 'XLE', 'UCO', 'UPST', 'SMCI', 'ENVX', 'HIMS', 'HAL', 'MARA', 'BILI', 'CSX', 'GLXY', 'STZ', 'STAA', 'MSFT', 'OPCH', 'BKR', 'RCL', 'PRTA', 'NVDL', 'XLY']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['HQC on $INTC with 3702 quote changes resulting in 1353 trades  @ 2025-07-25 07:00:00. Last appeared on 2025-07-24 16:05:33, 1 days ago. Trade size is $4,202,784.00 @ rank 2196 for Size out of 24210 occurrences.',\n",
       "  'HQC on $PHIO with 4192 quote changes resulting in 962 trades  @ 2025-07-25 07:57:00. Last appeared on 2025-05-14 12:34:58, 72 days ago. Trade size is $467,537.00 @ rank 79 for Size out of 213 occurrences.',\n",
       "  'HQC on $QQQ with 4947 quote changes resulting in 316 trades  @ 2025-07-25 09:30:00. Last appeared on 2025-07-24 16:01:00, 1 days ago. Trade size is $64,284,132.00 @ rank 935 for Size out of 2156456 occurrences.',\n",
       "  'HQC on $IWM with 4854 quote changes resulting in 508 trades  @ 2025-07-25 09:30:00. Last appeared on 2025-07-24 16:00:00, 1 days ago. Trade size is $27,099,153.00 @ rank 888 for Size out of 508917 occurrences.',\n",
       "  'HQC on $QS with 3772 quote changes resulting in 1569 trades  @ 2025-07-25 09:30:06. Last appeared on 2025-07-24 15:57:34, 1 days ago. Trade size is $11,052,352.00 @ rank 15 for Size out of 2433 occurrences.',\n",
       "  'HQC on $ASTS with 3563 quote changes resulting in 742 trades  @ 2025-07-25 09:30:11. Last appeared on 2025-07-24 15:40:10, 1 days ago. Trade size is $2,543,909.00 @ rank 301 for Size out of 2812 occurrences.',\n",
       "  'HQC on $QBTS with 7652 quote changes resulting in 1350 trades  @ 2025-07-25 09:33:05. Last appeared on 2025-07-24 15:59:46, 1 days ago. Trade size is $3,286,351.00 @ rank 810 for Size out of 17415 occurrences.',\n",
       "  'HQC on $IOVA with 9752 quote changes resulting in 2253 trades  @ 2025-07-25 09:33:55. Last appeared on 2025-07-24 12:54:42, 1 days ago. Trade size is $2,225,573.00 @ rank 7 for Size out of 625 occurrences.',\n",
       "  'HQC on $MU with 3775 quote changes resulting in 1312 trades  @ 2025-07-25 09:38:17. Last appeared on 2025-07-24 15:54:50, 1 days ago. Trade size is $14,497,247.00 @ rank 133 for Size out of 4624 occurrences.',\n",
       "  'HQC on $OPEN with 3547 quote changes resulting in 1171 trades  @ 2025-07-25 09:39:00. Last appeared on 2025-07-24 15:56:38, 1 days ago. Trade size is $2,896,020.00 @ rank 239 for Size out of 8237 occurrences.',\n",
       "  'HQC on $FLG with 4911 quote changes resulting in 1608 trades  @ 2025-07-25 09:40:11. Last appeared on 2025-07-23 15:14:46, 2 days ago. Trade size is $1,726,087.00 @ rank 21 for Size out of 400 occurrences.',\n",
       "  'HQC on $IREN with 3825 quote changes resulting in 615 trades  @ 2025-07-25 09:41:43. Last appeared on 2025-07-24 13:57:32, 1 days ago. Trade size is $1,258,863.00 @ rank 568 for Size out of 5536 occurrences.',\n",
       "  'HQC on $ACHR with 3520 quote changes resulting in 746 trades  @ 2025-07-25 09:42:24. Last appeared on 2025-07-24 15:12:57, 1 days ago. Trade size is $1,687,407.00 @ rank 704 for Size out of 3678 occurrences.',\n",
       "  'HQC on $CMCSA with 5524 quote changes resulting in 999 trades  @ 2025-07-25 09:43:14. Last appeared on 2025-07-24 10:09:50, 1 days ago. Trade size is $4,840,093.00 @ rank 150 for Size out of 2804 occurrences.',\n",
       "  'HQC on $APA with 3506 quote changes resulting in 631 trades  @ 2025-07-25 09:46:30. Last appeared on 2025-07-24 11:02:22, 1 days ago. Trade size is $1,386,012.00 @ rank 192 for Size out of 1135 occurrences.',\n",
       "  'HQC on $SPY with 4547 quote changes resulting in 269 trades  @ 2025-07-25 09:50:21. Last appeared on 2025-07-24 16:00:07, 1 days ago. Trade size is $25,372,468.00 @ rank 15087 for Size out of 1598328 occurrences.',\n",
       "  'HQC on $NVDA with 4042 quote changes resulting in 3596 trades  @ 2025-07-25 09:54:01. Last appeared on 2025-07-24 15:59:59, 1 days ago. Trade size is $80,820,400.00 @ rank 990 for Size out of 1016720 occurrences.',\n",
       "  'HQC on $FAST with 4001 quote changes resulting in 860 trades  @ 2025-07-25 09:54:06. Last appeared on 2025-07-24 14:36:28, 1 days ago. Trade size is $5,789,698.00 @ rank 7 for Size out of 251 occurrences.',\n",
       "  'HQC on $DHI with 4170 quote changes resulting in 52 trades  @ 2025-07-25 09:55:08. Last appeared on 2025-07-24 14:23:10, 1 days ago. Trade size is $277,690.00 @ rank 139 for Size out of 652 occurrences.',\n",
       "  'HQC on $KMI with 3540 quote changes resulting in 670 trades  @ 2025-07-25 10:06:46. Last appeared on 2025-07-24 15:59:56, 1 days ago. Trade size is $2,947,619.00 @ rank 162 for Size out of 1399 occurrences.',\n",
       "  'HQC on $XLK with 3903 quote changes resulting in 1096 trades  @ 2025-07-25 10:07:01. Last appeared on 2025-07-24 15:59:00, 1 days ago. Trade size is $14,863,082.00 @ rank 261 for Size out of 35821 occurrences.',\n",
       "  'HQC on $SOXL with 4355 quote changes resulting in 1211 trades  @ 2025-07-25 10:07:01. Last appeared on 2025-07-24 15:59:55, 1 days ago. Trade size is $10,905,364.00 @ rank 372 for Size out of 338286 occurrences.',\n",
       "  'HQC on $ATAI with 3960 quote changes resulting in 1068 trades  @ 2025-07-25 10:22:30. Last appeared on 2025-07-24 15:29:59, 1 days ago. Trade size is $1,100,249.00 @ rank 1 for Size out of 202 occurrences.',\n",
       "  'HQC on $AAPL with 4458 quote changes resulting in 2126 trades  @ 2025-07-25 10:34:21. Last appeared on 2025-07-24 15:59:58, 1 days ago. Trade size is $39,137,335.00 @ rank 388 for Size out of 29730 occurrences.',\n",
       "  'HQC on $XLI with 3515 quote changes resulting in 591 trades  @ 2025-07-25 10:34:21. Last appeared on 2025-07-24 15:59:56, 1 days ago. Trade size is $12,035,054.00 @ rank 403 for Size out of 70775 occurrences.',\n",
       "  'HQC on $TQQQ with 7279 quote changes resulting in 1341 trades  @ 2025-07-25 10:34:21. Last appeared on 2025-07-24 16:00:00, 1 days ago. Trade size is $24,741,990.00 @ rank 209 for Size out of 2126127 occurrences.',\n",
       "  'HQC on $PTC with 4364 quote changes resulting in 3 trades  @ 2025-07-25 10:47:14. Last appeared on 2025-07-09 15:55:59, 16 days ago. Trade size is $29,457.00 @ rank 22 for Size out of 31 occurrences.',\n",
       "  'HQC on $IVZ with 3511 quote changes resulting in 808 trades  @ 2025-07-25 10:52:14. Last appeared on 2025-07-23 11:22:32, 2 days ago. Trade size is $2,324,317.00 @ rank 20 for Size out of 527 occurrences.',\n",
       "  'HQC on $LBRT with 5226 quote changes resulting in 1234 trades  @ 2025-07-25 10:53:10. Last appeared on 2025-07-24 13:41:36, 1 days ago. Trade size is $2,517,300.00 @ rank 2 for Size out of 128 occurrences.',\n",
       "  'HQC on $YINN with 3551 quote changes resulting in 810 trades  @ 2025-07-25 10:57:10. Last appeared on 2025-07-24 15:55:16, 1 days ago. Trade size is $3,284,997.00 @ rank 78 for Size out of 3829 occurrences.',\n",
       "  'HQC on $IGV with 3732 quote changes resulting in 1524 trades  @ 2025-07-25 11:08:24. Last appeared on 2025-07-24 15:55:00, 1 days ago. Trade size is $27,726,663.00 @ rank 3 for Size out of 3266 occurrences.',\n",
       "  'HQC on $LIDR with 4882 quote changes resulting in 2674 trades  @ 2025-07-25 11:15:11. Last appeared on 2025-07-24 15:46:17, 1 days ago. Trade size is $3,522,969.00 @ rank 1 for Size out of 1063 occurrences.',\n",
       "  'HQC on $NEM with 4991 quote changes resulting in 1175 trades  @ 2025-07-25 11:15:40. Last appeared on 2025-07-24 15:59:03, 1 days ago. Trade size is $5,291,722.00 @ rank 79 for Size out of 2516 occurrences.',\n",
       "  'HQC on $MBLY with 3527 quote changes resulting in 1080 trades  @ 2025-07-25 11:28:52. Last appeared on 2025-07-24 15:30:34, 1 days ago. Trade size is $4,203,735.00 @ rank 7 for Size out of 538 occurrences.',\n",
       "  'HQC on $POR with 9104 quote changes resulting in 2914 trades  @ 2025-07-25 11:31:47. Last appeared on 2025-07-23 14:19:24, 2 days ago. Trade size is $5,649,526.00 @ rank 1 for Size out of 9 occurrences.',\n",
       "  'HQC on $BBWI with 4357 quote changes resulting in 1365 trades  @ 2025-07-25 11:34:39. Last appeared on 2025-07-24 11:44:02, 1 days ago. Trade size is $3,100,547.00 @ rank 4 for Size out of 298 occurrences.',\n",
       "  'HQC on $OKLO with 4595 quote changes resulting in 1124 trades  @ 2025-07-25 11:34:41. Last appeared on 2025-07-24 15:44:10, 1 days ago. Trade size is $2,931,481.00 @ rank 667 for Size out of 2643 occurrences.',\n",
       "  'HQC on $USO with 8575 quote changes resulting in 923 trades  @ 2025-07-25 11:38:00. Last appeared on 2025-07-24 14:28:06, 1 days ago. Trade size is $6,853,747.00 @ rank 331 for Size out of 39816 occurrences.',\n",
       "  'HQC on $XLE with 3529 quote changes resulting in 350 trades  @ 2025-07-25 11:38:00. Last appeared on 2025-07-24 15:55:19, 1 days ago. Trade size is $3,046,534.00 @ rank 4131 for Size out of 45647 occurrences.',\n",
       "  'HQC on $UCO with 5829 quote changes resulting in 633 trades  @ 2025-07-25 11:38:00. Last appeared on 2025-07-24 14:28:05, 1 days ago. Trade size is $1,968,331.00 @ rank 190 for Size out of 6240 occurrences.',\n",
       "  'HQC on $UPST with 3936 quote changes resulting in 865 trades  @ 2025-07-25 11:52:52. Last appeared on 2025-07-23 11:00:31, 3 days ago. Trade size is $3,912,405.00 @ rank 71 for Size out of 487 occurrences.',\n",
       "  'HQC on $SMCI with 3539 quote changes resulting in 723 trades  @ 2025-07-25 11:55:09. Last appeared on 2025-07-24 15:55:10, 1 days ago. Trade size is $4,764,655.00 @ rank 2808 for Size out of 49497 occurrences.',\n",
       "  'HQC on $ENVX with 3754 quote changes resulting in 710 trades  @ 2025-07-25 11:58:13. Last appeared on 2025-07-24 15:32:39, 1 days ago. Trade size is $1,052,200.00 @ rank 133 for Size out of 1354 occurrences.',\n",
       "  'HQC on $HIMS with 6210 quote changes resulting in 1594 trades  @ 2025-07-25 12:00:04. Last appeared on 2025-07-24 15:42:11, 1 days ago. Trade size is $12,791,955.00 @ rank 35 for Size out of 3428 occurrences.',\n",
       "  'HQC on $HAL with 3724 quote changes resulting in 769 trades  @ 2025-07-25 12:26:38. Last appeared on 2025-07-24 15:56:06, 1 days ago. Trade size is $3,163,735.00 @ rank 135 for Size out of 1894 occurrences.',\n",
       "  'HQC on $MARA with 5108 quote changes resulting in 763 trades  @ 2025-07-25 12:45:02. Last appeared on 2025-07-24 15:30:49, 1 days ago. Trade size is $2,764,996.00 @ rank 874 for Size out of 23156 occurrences.',\n",
       "  'HQC on $BILI with 3795 quote changes resulting in 845 trades  @ 2025-07-25 12:48:29. Last appeared on 2025-07-24 11:37:04, 2 days ago. Trade size is $1,796,634.00 @ rank 120 for Size out of 1321 occurrences.',\n",
       "  'HQC on $CSX with 3543 quote changes resulting in 830 trades  @ 2025-07-25 13:15:52. Last appeared on 2025-07-24 15:51:37, 1 days ago. Trade size is $7,809,023.00 @ rank 27 for Size out of 2749 occurrences.',\n",
       "  'HQC on $GLXY with 4162 quote changes resulting in 722 trades  @ 2025-07-25 13:37:29. Last appeared on 2025-07-24 15:12:04, 1 days ago. Trade size is $2,012,695.00 @ rank 20 for Size out of 316 occurrences.',\n",
       "  'HQC on $STZ with 6087 quote changes resulting in 2 trades  @ 2025-07-25 13:43:57. Last appeared on 2025-07-22 12:19:22, 4 days ago. Trade size is $11,714.00 @ rank 182 for Size out of 218 occurrences.',\n",
       "  'HQC on $STAA with 3760 quote changes resulting in 1597 trades  @ 2025-07-25 14:07:59. Last appeared on 2025-07-23 15:58:27, 2 days ago. Trade size is $2,445,717.00 @ rank 3 for Size out of 74 occurrences.',\n",
       "  'HQC on $MSFT with 4454 quote changes resulting in 3409 trades  @ 2025-07-25 14:10:21. Last appeared on 2025-07-24 15:59:49, 1 days ago. Trade size is $71,834,590.00 @ rank 98 for Size out of 3302 occurrences.',\n",
       "  'HQC on $OPCH with 4987 quote changes resulting in 29 trades  @ 2025-07-25 14:24:34. Last appeared on 2025-07-08 12:58:57, 18 days ago. Trade size is $42,917.00 @ rank 52 for Size out of 57 occurrences.',\n",
       "  'HQC on $BKR with 3522 quote changes resulting in 710 trades  @ 2025-07-25 14:47:18. Last appeared on 2025-07-24 15:53:32, 1 days ago. Trade size is $2,534,872.00 @ rank 186 for Size out of 1441 occurrences.',\n",
       "  'HQC on $RCL with 3660 quote changes resulting in 1763 trades  @ 2025-07-25 15:21:11. Last appeared on 2025-07-22 09:33:00, 4 days ago. Trade size is $15,077,046.00 @ rank 1 for Size out of 50 occurrences.',\n",
       "  'HQC on $PRTA with 3549 quote changes resulting in 865 trades  @ 2025-07-25 15:41:48. Last appeared on 2025-07-23 15:54:27, 2 days ago. Trade size is $643,208.00 @ rank 5 for Size out of 67 occurrences.',\n",
       "  'HQC on $NVDL with 4147 quote changes resulting in 438 trades  @ 2025-07-25 15:54:40. Last appeared on 2025-07-24 15:55:00, 1 days ago. Trade size is $3,560,351.00 @ rank 1334 for Size out of 103058 occurrences.',\n",
       "  'HQC on $XLY with 3643 quote changes resulting in 470 trades  @ 2025-07-25 15:55:00. Last appeared on 2025-07-24 15:59:55, 1 days ago. Trade size is $8,619,385.00 @ rank 193 for Size out of 10972 occurrences.'],\n",
       " ['INTC',\n",
       "  'PHIO',\n",
       "  'QQQ',\n",
       "  'IWM',\n",
       "  'QS',\n",
       "  'ASTS',\n",
       "  'QBTS',\n",
       "  'IOVA',\n",
       "  'MU',\n",
       "  'OPEN',\n",
       "  'FLG',\n",
       "  'IREN',\n",
       "  'ACHR',\n",
       "  'CMCSA',\n",
       "  'APA',\n",
       "  'SPY',\n",
       "  'NVDA',\n",
       "  'FAST',\n",
       "  'DHI',\n",
       "  'KMI',\n",
       "  'XLK',\n",
       "  'SOXL',\n",
       "  'ATAI',\n",
       "  'AAPL',\n",
       "  'XLI',\n",
       "  'TQQQ',\n",
       "  'PTC',\n",
       "  'IVZ',\n",
       "  'LBRT',\n",
       "  'YINN',\n",
       "  'IGV',\n",
       "  'LIDR',\n",
       "  'NEM',\n",
       "  'MBLY',\n",
       "  'POR',\n",
       "  'BBWI',\n",
       "  'OKLO',\n",
       "  'USO',\n",
       "  'XLE',\n",
       "  'UCO',\n",
       "  'UPST',\n",
       "  'SMCI',\n",
       "  'ENVX',\n",
       "  'HIMS',\n",
       "  'HAL',\n",
       "  'MARA',\n",
       "  'BILI',\n",
       "  'CSX',\n",
       "  'GLXY',\n",
       "  'STZ',\n",
       "  'STAA',\n",
       "  'MSFT',\n",
       "  'OPCH',\n",
       "  'BKR',\n",
       "  'RCL',\n",
       "  'PRTA',\n",
       "  'NVDL',\n",
       "  'XLY'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=load_data(parquet_path)\n",
    "#filter for NUmquotes greater that 3500\n",
    "if df is None:\n",
    "    df = pl.DataFrame() \n",
    "else:\n",
    "    df = df.filter(pl.col('NumQuotes') > 3500)\n",
    "#     #sort top 5 by dollars\n",
    "#     # df = df.sort('Dollars', descending=True).head(5)\n",
    "#     # sort by uniques symbols with most occurances\n",
    "#     # df = df.group_by('Symbol').agg(pl.col('Dollars').sum().alias('Total_Dollars')).sort('Total_Dollars', descending=True).head(5)   \n",
    "df\n",
    "\n",
    "# df1=load_data('./Processed_Data/burst_data_processed.parquet')\n",
    "# df1\n",
    "\n",
    "generate_alert_strings(df1, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8f114b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (60, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Symbol</th><th>T1</th><th>Event_Date_Time</th><th>NumQuotes</th><th>BBO</th><th>NumTrades</th><th>Dollars</th><th>T3</th><th>Trade_Size</th></tr><tr><td>str</td><td>str</td><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;INTC&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 07:00:00</td><td>3702</td><td>1421</td><td>1353</td><td>4.202784e6</td><td>null</td><td>3106.27051</td></tr><tr><td>&quot;PHIO&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 07:57:00</td><td>4192</td><td>1538</td><td>962</td><td>467537.0</td><td>null</td><td>486.005198</td></tr><tr><td>&quot;PHIO&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 07:57:01</td><td>3624</td><td>1264</td><td>778</td><td>303421.0</td><td>null</td><td>390.001285</td></tr><tr><td>&quot;PHIO&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 07:57:29</td><td>3543</td><td>1369</td><td>894</td><td>502219.0</td><td>null</td><td>561.766219</td></tr><tr><td>&quot;QQQ&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 09:30:00</td><td>4947</td><td>940</td><td>316</td><td>6.4284132e7</td><td>null</td><td>203430.797468</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;LIDR&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 11:15:19</td><td>3549</td><td>1359</td><td>697</td><td>967068.0</td><td>null</td><td>1387.472023</td></tr><tr><td>&quot;NEM&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 11:15:40</td><td>4991</td><td>1026</td><td>1175</td><td>5.291722e6</td><td>null</td><td>4503.593191</td></tr><tr><td>&quot;NVDA&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 11:17:00</td><td>3833</td><td>1001</td><td>894</td><td>1.510261e7</td><td>null</td><td>16893.299776</td></tr><tr><td>&quot;LIDR&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 11:27:47</td><td>4075</td><td>1304</td><td>999</td><td>1.317375e6</td><td>null</td><td>1318.693694</td></tr><tr><td>&quot;MBLY&quot;</td><td>&quot;0&quot;</td><td>2025-07-25 11:28:52</td><td>3527</td><td>1090</td><td>1080</td><td>4.203735e6</td><td>null</td><td>3892.347222</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (60, 9)\n",
       "┌────────┬─────┬──────────────────┬───────────┬───┬───────────┬─────────────┬──────┬───────────────┐\n",
       "│ Symbol ┆ T1  ┆ Event_Date_Time  ┆ NumQuotes ┆ … ┆ NumTrades ┆ Dollars     ┆ T3   ┆ Trade_Size    │\n",
       "│ ---    ┆ --- ┆ ---              ┆ ---       ┆   ┆ ---       ┆ ---         ┆ ---  ┆ ---           │\n",
       "│ str    ┆ str ┆ datetime[μs]     ┆ i64       ┆   ┆ i64       ┆ f64         ┆ str  ┆ f64           │\n",
       "╞════════╪═════╪══════════════════╪═══════════╪═══╪═══════════╪═════════════╪══════╪═══════════════╡\n",
       "│ INTC   ┆ 0   ┆ 2025-07-25       ┆ 3702      ┆ … ┆ 1353      ┆ 4.202784e6  ┆ null ┆ 3106.27051    │\n",
       "│        ┆     ┆ 07:00:00         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ PHIO   ┆ 0   ┆ 2025-07-25       ┆ 4192      ┆ … ┆ 962       ┆ 467537.0    ┆ null ┆ 486.005198    │\n",
       "│        ┆     ┆ 07:57:00         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ PHIO   ┆ 0   ┆ 2025-07-25       ┆ 3624      ┆ … ┆ 778       ┆ 303421.0    ┆ null ┆ 390.001285    │\n",
       "│        ┆     ┆ 07:57:01         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ PHIO   ┆ 0   ┆ 2025-07-25       ┆ 3543      ┆ … ┆ 894       ┆ 502219.0    ┆ null ┆ 561.766219    │\n",
       "│        ┆     ┆ 07:57:29         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ QQQ    ┆ 0   ┆ 2025-07-25       ┆ 4947      ┆ … ┆ 316       ┆ 6.4284132e7 ┆ null ┆ 203430.797468 │\n",
       "│        ┆     ┆ 09:30:00         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ …      ┆ …   ┆ …                ┆ …         ┆ … ┆ …         ┆ …           ┆ …    ┆ …             │\n",
       "│ LIDR   ┆ 0   ┆ 2025-07-25       ┆ 3549      ┆ … ┆ 697       ┆ 967068.0    ┆ null ┆ 1387.472023   │\n",
       "│        ┆     ┆ 11:15:19         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ NEM    ┆ 0   ┆ 2025-07-25       ┆ 4991      ┆ … ┆ 1175      ┆ 5.291722e6  ┆ null ┆ 4503.593191   │\n",
       "│        ┆     ┆ 11:15:40         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ NVDA   ┆ 0   ┆ 2025-07-25       ┆ 3833      ┆ … ┆ 894       ┆ 1.510261e7  ┆ null ┆ 16893.299776  │\n",
       "│        ┆     ┆ 11:17:00         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ LIDR   ┆ 0   ┆ 2025-07-25       ┆ 4075      ┆ … ┆ 999       ┆ 1.317375e6  ┆ null ┆ 1318.693694   │\n",
       "│        ┆     ┆ 11:27:47         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "│ MBLY   ┆ 0   ┆ 2025-07-25       ┆ 3527      ┆ … ┆ 1080      ┆ 4.203735e6  ┆ null ┆ 3892.347222   │\n",
       "│        ┆     ┆ 11:28:52         ┆           ┆   ┆           ┆             ┆      ┆               │\n",
       "└────────┴─────┴──────────────────┴───────────┴───┴───────────┴─────────────┴──────┴───────────────┘"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377f9cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_hft1_stream(most_recent_file,output_file,topic,client,last_size):\n",
    "    global current_time,formatted_date,c\n",
    "    # Load the last size from a pickle file if it exists\n",
    "    progress_file= f'./Processed_data/{formatted_date}_HFT1_last_size.pkl'\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'rb') as f:\n",
    "            last_size = pickle.load(f)\n",
    "            #print(f\"Last Size: {last_size}\")\n",
    "\n",
    "    # Check if the file has changed\n",
    "    current_size = os.stat(most_recent_file).st_size\n",
    "    min_time=None\n",
    "    row_dict = {}\n",
    "\n",
    "    if current_size != last_size:\n",
    "        current_time = datetime.now()\n",
    "        # The file has changed, open it and read from the point where we left off\n",
    "        with open(most_recent_file, 'r') as f:\n",
    "            # Seek to the position of the last read\n",
    "            f.seek(last_size)\n",
    "            # Read the new lines\n",
    "            new_lines = f.readlines()\n",
    "\n",
    "        # Process the new lines into JSON and write them to a new file\n",
    "        with open(output_file, 'a') as f1:\n",
    "            for line in csv.reader(new_lines):\n",
    "                # Create a dictionary for each line\n",
    "                row_dict = dict(zip(['Symbol', 'T1', 'Event_Date_Time', 'NumQuotes', 'BBO', 'NumTrades', 'Dollars', 'T3'], line))  \n",
    "                row_dict['Trade_Size'] = None\n",
    "                # Convert 'Event_Date_Time' from int to time\n",
    "                row_dict['Event_Date_Time'] = int_to_time(row_dict['Event_Date_Time'])\n",
    "                # Combine 'Event_Date_Time' with temp_date\n",
    "                temp_date = extract_date_from_filename_datetime(most_recent_file)\n",
    "                row_dict['Event_Date_Time'] = datetime.combine(temp_date, row_dict['Event_Date_Time'])\n",
    "                # Convert 'Event_Date_Time' to a string in ISO 8601 format\n",
    "                row_dict['Event_Date_Time'] = row_dict['Event_Date_Time'].isoformat()\n",
    "\n",
    "                if min_time is None:\n",
    "                    min_time = datetime.fromisoformat(row_dict['Event_Date_Time'])\n",
    "\n",
    "                try:\n",
    "                    r = c.get_quote(row_dict['Symbol'] ,fields=[c.Quote.Fields.QUOTE,c.Quote.Fields.REGULAR,c.Quote.Fields.EXTENDED])\n",
    "                    assert r.status_code == 200, r.raise_for_status()\n",
    "                    new_data = r.json()\n",
    "                    # print(new_data)\n",
    "                    row_dict['Price'] = new_data[row_dict['Symbol']]['regular']['regularMarketLastPrice']\n",
    "                    row_dict['Size'] = new_data[row_dict['Symbol']]['regular']['regularMarketLastSize']\n",
    "                    row_dict['Volume'] = new_data[row_dict['Symbol']]['quote']['totalVolume']\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred: {e}\")\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    row_dict['Trade_Size'] = float(row_dict['Dollars']) / float(row_dict['NumTrades'])\n",
    "                except ZeroDivisionError:\n",
    "                    row_dict['Trade_Size'] = 0\n",
    "                                \n",
    "                # Convert the dictionary to a JSON string\n",
    "                row_dict_str = json.dumps(row_dict)\n",
    "                                \n",
    "                # Write the dictionary as a JSON object to the file\n",
    "                f1.write(row_dict_str+ '\\n')\n",
    "                client.publish(\"hft1_data\", row_dict_str)\n",
    "            \n",
    "        # Update the last size\n",
    "        last_size = current_size\n",
    "        # Save the last size to a pickle file\n",
    "        with open(progress_file, 'wb') as f:\n",
    "            pickle.dump(last_size, f)\n",
    "\n",
    "        if 'Event_Date_Time' not in row_dict:\n",
    "            return  last_size\n",
    "\n",
    "        max_time = datetime.fromisoformat(row_dict['Event_Date_Time'])\n",
    "        # Calculate the time difference\n",
    "        time_diff = max_time - min_time\n",
    "        # Get the number of lines\n",
    "        num_lines = len(new_lines)\n",
    "        try:\n",
    "            eps = num_lines / time_diff.total_seconds()\n",
    "        except ZeroDivisionError:\n",
    "            eps = 0\n",
    "        min_time=None  \n",
    "\n",
    "        client.publish(topic, f\"{eps}\")\n",
    "\n",
    "    # Check if 120 seconds has elapsed since the last time current_time was updated\n",
    "    if datetime.now() - current_time >= timedelta(seconds=120):\n",
    "        print(\"120 seconds has elapsed since the last update of current_time.\")\n",
    "        current_time = datetime.now()  # Update current_time\n",
    "\n",
    "    return last_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae467999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "import io\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_date_from_filename_datetime(filename):\n",
    "    import re\n",
    "    date = re.search(r'\\d{8}', filename)\n",
    "    if date:\n",
    "        return datetime.strptime(date.group(0), '%Y%m%d')\n",
    "    return None\n",
    "\n",
    "def int_to_time(time_int):\n",
    "    time_str = str(time_int).zfill(6)\n",
    "    hour = int(time_str[:2])\n",
    "    minute = int(time_str[2:4])\n",
    "    second = int(time_str[4:])\n",
    "    from datetime import time\n",
    "    return time(hour=hour, minute=minute, second=second)\n",
    "\n",
    "def monitor_and_update_hft1_parquet(csv_path, parquet_path, last_size_path, schema):\n",
    "    # Load last read size\n",
    "    last_size = 0\n",
    "    if os.path.exists(last_size_path):\n",
    "        with open(last_size_path, 'rb') as f:\n",
    "            last_size = pickle.load(f)\n",
    "\n",
    "    # Get current file size\n",
    "    current_size = os.stat(csv_path).st_size\n",
    "\n",
    "    # If no new data, just return the current parquet DataFrame (if exists)\n",
    "    if current_size == last_size and os.path.exists(parquet_path):\n",
    "        return pl.read_parquet(parquet_path)\n",
    "\n",
    "    # Read only new lines\n",
    "    with open(csv_path, 'r') as f:\n",
    "        f.seek(last_size)\n",
    "        new_lines = f.readlines()\n",
    "\n",
    "    # If no new lines, return current parquet DataFrame\n",
    "    if not new_lines:\n",
    "        if os.path.exists(parquet_path):\n",
    "            return pl.read_parquet(parquet_path)\n",
    "        else:\n",
    "            return pl.DataFrame(schema=schema)\n",
    "\n",
    "    # Create DataFrame from new lines\n",
    "    new_df = pl.read_csv(\n",
    "        source=io.StringIO(''.join(new_lines)),  # <-- fix here\n",
    "        has_header=False,\n",
    "        schema=schema\n",
    "    )\n",
    "\n",
    "    # Extract date from filename\n",
    "    temp_date = extract_date_from_filename_datetime(csv_path)\n",
    "\n",
    "    # Convert Event_Date_Time from int to time, then to datetime, then to ISO string\n",
    "    new_df = new_df.with_columns([\n",
    "        pl.col('Event_Date_Time').map_elements(lambda x: int_to_time(int(x)), return_dtype=pl.Time).alias('Event_Date_Time')\n",
    "    ])\n",
    "    new_df = new_df.with_columns([\n",
    "        pl.col('Event_Date_Time').map_elements(lambda x: datetime.combine(temp_date, x), return_dtype=pl.Datetime).alias('Event_Date_Time')\n",
    "    ])\n",
    "\n",
    "    # Calculate Trade_Size\n",
    "    new_df = new_df.with_columns([\n",
    "        (pl.col('Dollars') / pl.col('NumTrades')).alias('Trade_Size')\n",
    "    ])\n",
    "\n",
    "    # Drop rows with NaN in 'Dollars'\n",
    "    new_df = new_df.drop_nulls(subset=['Dollars'])\n",
    "\n",
    "    # Sort by Event_Date_Time\n",
    "    new_df = new_df.sort('Event_Date_Time')\n",
    "\n",
    "    # If parquet file exists, append new data\n",
    "    if os.path.exists(parquet_path):\n",
    "        old_df = pl.read_parquet(parquet_path)\n",
    "        full_df = pl.concat([old_df, new_df])\n",
    "    else:\n",
    "        full_df = new_df\n",
    "\n",
    "    # Save updated DataFrame to parquet\n",
    "    full_df.write_parquet(parquet_path)\n",
    "\n",
    "    # Update last size\n",
    "    with open(last_size_path, 'wb') as f:\n",
    "        pickle.dump(current_size, f)\n",
    "\n",
    "    return full_df\n",
    "\n",
    "schema = {\n",
    "    \"Symbol\": pl.Utf8,\n",
    "    \"T1\": pl.Utf8,\n",
    "    \"Event_Date_Time\": pl.Utf8,\n",
    "    \"NumQuotes\": pl.Int64,\n",
    "    \"BBO\": pl.Int64,\n",
    "    \"NumTrades\": pl.Int64,\n",
    "    \"Dollars\": pl.Float64,\n",
    "    \"T3\": pl.Utf8\n",
    "}\n",
    "df = monitor_and_update_hft1_parquet(\n",
    "    csv_path=\"./Data/20250722_hft1.csv\",\n",
    "    parquet_path=\"./Processed_Data/20250722_hft1.parquet\",\n",
    "    last_size_path=\"./Data/20250722_hft1_lastsize.pkl\",\n",
    "    schema=schema\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826bc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (110_336, 9)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Symbol</th><th>T1</th><th>Event_Date_Time</th><th>NumQuotes</th><th>BBO</th><th>NumTrades</th><th>Dollars</th><th>T3</th><th>Trade_Size</th></tr><tr><td>str</td><td>str</td><td>datetime[μs]</td><td>i64</td><td>i64</td><td>i64</td><td>f64</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;OSCR&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 06:00:00</td><td>1118</td><td>502</td><td>761</td><td>884167.0</td><td>null</td><td>1161.848883</td></tr><tr><td>&quot;OPEN&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 07:00:00</td><td>2411</td><td>1093</td><td>681</td><td>959758.0</td><td>null</td><td>1409.33627</td></tr><tr><td>&quot;IVF&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 07:00:04</td><td>1014</td><td>278</td><td>289</td><td>166448.0</td><td>null</td><td>575.944637</td></tr><tr><td>&quot;IVF&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 07:00:05</td><td>1509</td><td>534</td><td>486</td><td>144934.0</td><td>null</td><td>298.218107</td></tr><tr><td>&quot;IVF&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 07:00:06</td><td>1131</td><td>463</td><td>468</td><td>105815.0</td><td>null</td><td>226.100427</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;QQQ&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 16:04:31</td><td>1506</td><td>309</td><td>15</td><td>722136.0</td><td>null</td><td>48142.4</td></tr><tr><td>&quot;ENPH&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 16:05:00</td><td>1357</td><td>299</td><td>572</td><td>816241.0</td><td>null</td><td>1426.994755</td></tr><tr><td>&quot;SOXS&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 16:08:17</td><td>1135</td><td>399</td><td>261</td><td>410666.0</td><td>null</td><td>1573.43295</td></tr><tr><td>&quot;SOXS&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 16:10:00</td><td>1387</td><td>355</td><td>104</td><td>383240.0</td><td>null</td><td>3685.0</td></tr><tr><td>&quot;SOXS&quot;</td><td>&quot;0&quot;</td><td>2025-07-22 16:18:52</td><td>1520</td><td>253</td><td>65</td><td>170538.0</td><td>null</td><td>2623.661538</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (110_336, 9)\n",
       "┌────────┬─────┬─────────────────────┬───────────┬───┬───────────┬──────────┬──────┬─────────────┐\n",
       "│ Symbol ┆ T1  ┆ Event_Date_Time     ┆ NumQuotes ┆ … ┆ NumTrades ┆ Dollars  ┆ T3   ┆ Trade_Size  │\n",
       "│ ---    ┆ --- ┆ ---                 ┆ ---       ┆   ┆ ---       ┆ ---      ┆ ---  ┆ ---         │\n",
       "│ str    ┆ str ┆ datetime[μs]        ┆ i64       ┆   ┆ i64       ┆ f64      ┆ str  ┆ f64         │\n",
       "╞════════╪═════╪═════════════════════╪═══════════╪═══╪═══════════╪══════════╪══════╪═════════════╡\n",
       "│ OSCR   ┆ 0   ┆ 2025-07-22 06:00:00 ┆ 1118      ┆ … ┆ 761       ┆ 884167.0 ┆ null ┆ 1161.848883 │\n",
       "│ OPEN   ┆ 0   ┆ 2025-07-22 07:00:00 ┆ 2411      ┆ … ┆ 681       ┆ 959758.0 ┆ null ┆ 1409.33627  │\n",
       "│ IVF    ┆ 0   ┆ 2025-07-22 07:00:04 ┆ 1014      ┆ … ┆ 289       ┆ 166448.0 ┆ null ┆ 575.944637  │\n",
       "│ IVF    ┆ 0   ┆ 2025-07-22 07:00:05 ┆ 1509      ┆ … ┆ 486       ┆ 144934.0 ┆ null ┆ 298.218107  │\n",
       "│ IVF    ┆ 0   ┆ 2025-07-22 07:00:06 ┆ 1131      ┆ … ┆ 468       ┆ 105815.0 ┆ null ┆ 226.100427  │\n",
       "│ …      ┆ …   ┆ …                   ┆ …         ┆ … ┆ …         ┆ …        ┆ …    ┆ …           │\n",
       "│ QQQ    ┆ 0   ┆ 2025-07-22 16:04:31 ┆ 1506      ┆ … ┆ 15        ┆ 722136.0 ┆ null ┆ 48142.4     │\n",
       "│ ENPH   ┆ 0   ┆ 2025-07-22 16:05:00 ┆ 1357      ┆ … ┆ 572       ┆ 816241.0 ┆ null ┆ 1426.994755 │\n",
       "│ SOXS   ┆ 0   ┆ 2025-07-22 16:08:17 ┆ 1135      ┆ … ┆ 261       ┆ 410666.0 ┆ null ┆ 1573.43295  │\n",
       "│ SOXS   ┆ 0   ┆ 2025-07-22 16:10:00 ┆ 1387      ┆ … ┆ 104       ┆ 383240.0 ┆ null ┆ 3685.0      │\n",
       "│ SOXS   ┆ 0   ┆ 2025-07-22 16:18:52 ┆ 1520      ┆ … ┆ 65        ┆ 170538.0 ┆ null ┆ 2623.661538 │\n",
       "└────────┴─────┴─────────────────────┴───────────┴───┴───────────┴──────────┴──────┴─────────────┘"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7238916",
   "metadata": {},
   "source": [
    "# Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776f5568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most recent NYSE open day is: 20250721\n"
     ]
    }
   ],
   "source": [
    "#Set start date for reading hft1 burst files\n",
    "data_start_date= '20240417'\n",
    "start_date = datetime.strptime(data_start_date, '%Y%m%d')\n",
    "\n",
    "# Get yesterday's date\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "\n",
    "# Create a calendar for NYSE\n",
    "nyse = mcal.get_calendar('NYSE')\n",
    "# Get the market schedule for the last 5 days up to today\n",
    "schedule = nyse.schedule(start_date=yesterday - timedelta(days=5), end_date=yesterday)\n",
    "\n",
    "# Find the most recent trading day\n",
    "previous_date = schedule.iloc[-1].name.strftime('%Y%m%d')\n",
    "data_end_date = datetime.strptime(previous_date, '%Y%m%d').replace(hour=16, minute=0, second=0)\n",
    "print(f\"The most recent NYSE open day is: {previous_date}\")\n",
    "\n",
    "#Set the end date to the most recent complete trading day\n",
    "end_date=data_end_date \n",
    "\n",
    "# Set the directory path where the CSV files are located. The application generated by this is intended to run in the directory that the burst monitor is running in and utilize\n",
    "# the same directory structure. The directory path is relative to the directory that the application is running in.\n",
    "\n",
    "directory_path = './Data/'\n",
    "\n",
    "# Set the file paths for the processed hft1 burst data and price data\n",
    "file_path = os.path.join(directory_path, 'burst_data_processed.parquet')\n",
    "# price_data_file_path = os.path.join(directory_path, 'price_data_processed_5.parquet')\n",
    "price_data_file_path = os.path.join(directory_path, 'price_data_processed.parquet')\n",
    "\n",
    "high_quote_threshold = 0\n",
    "active_high_quote_threshold = 3500\n",
    "files = os.listdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdc94a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afbee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell implements all of the code to monitor the incoming datafiles to determine when a change has been made to the most recent file and to trigger\n",
    "# a message based on that change. A dataframe containing all of the previous data must first be generated to provide the information for the message.\n",
    "\n",
    "# This code is designed to run after the Burst Monitor is activated for the day and has generated it's initial files for that day.\n",
    "\n",
    "# This function runs an endless loop until termianted by a keyboard interrupt from the user\n",
    "\n",
    "# Set the directory path where the CSV files are located. The application generated by this is intended to run in the directory that the burst monitor is running in and utilize\n",
    "# the same directory structure. The directory path is relative to the directory that the application is running in.\n",
    "\n",
    "active_high_quote_threshold = 3500\n",
    "directory_path = './Data/'\n",
    "\n",
    "# The following code is used to allow for picking up where the monitoring left off if it is interrupted if there have already been messages published.\n",
    "\n",
    "try:\n",
    "    len(published_msg)\n",
    "except NameError:\n",
    "    published_msg = []\n",
    "\n",
    "# Get a list of all files in the directory\n",
    "files = os.listdir(directory_path)\n",
    "# This identifies the file that is being modified today and uses it to determine when to trigger a message.\n",
    "# Filter the list to include only files that end with '.csv' and contain '_hft1' in the filename\n",
    "filtered_files = [f for f in files if f.endswith('.csv') and '_hft1' in f]\n",
    "filtered_files = sorted(filtered_files, key=lambda f: os.path.getmtime(os.path.join(directory_path, f)), reverse=True)\n",
    "most_recent_file=os.path.join(directory_path, filtered_files[0])\n",
    "\n",
    "# Set the file paths for the processed hft1 burst data and price data\n",
    "file_path = os.path.join(directory_path, 'burst_data_processed.parquet')\n",
    "concatenated_df=load_data(file_path)\n",
    "#Filter Concatenated Dataframe to only include data with NumQuotes greater than the axtive_high_quote_threshold\n",
    "concatenated_df = concatenated_df.filter(pl.col('NumQuotes') > active_high_quote_threshold)\n",
    "\n",
    "temp=concatenated_df.sort(by='Event_Date_Time', descending=False)\n",
    "\n",
    "#Remove Items from temp that have occurred today to prevent duplicate messages when starting server during the trading day\n",
    "\n",
    "today = datetime.today().date()\n",
    "today_date = datetime.today().strftime('%Y%m%d')\n",
    "# Filter the DataFrame to keep only rows where Event_Date_Time is not today\n",
    "temp = temp.filter(pl.col(\"Event_Date_Time\").dt.date() != today)\n",
    "\n",
    "# The complete dataframe is in temp and the following code neesd to monitor todays dataframe and update temp if changed, update it accordingly and generate the message\n",
    "\n",
    "with open(most_recent_file) as f:\n",
    "    #This is the first run through looking for messages for today -probably simpler to reread the whole data frame and process differences]\n",
    "    todays_events = make_df_from_high_quote_count(most_recent_file, active_high_quote_threshold)  \n",
    "    if not todays_events.is_empty():\n",
    "        msg_text, symbols = generate_alert_strings(temp,todays_events)\n",
    "        for msg in msg_text:\n",
    "            if msg not in published_msg:\n",
    "                display(Markdown(msg))\n",
    "                # send_slack_message('#burst_report',msg,slack_api_token)\n",
    "                # response = send_twitter_message(msg,service_name='KII')\n",
    "                # response = send_twitter_message(msg,service_name='HFT')\n",
    "                published_msg.append(msg)\n",
    "    else:\n",
    "        symbols = []\n",
    "        total_events = 0\n",
    "    f.close()\n",
    "\n",
    "#Mark the time that the file was last processed, the number of messages published and the length of todays_events\n",
    "last_modified = os.path.getmtime(most_recent_file)\n",
    "current_length=len(todays_events)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        current_modified = os.path.getmtime(most_recent_file)       \n",
    "        if current_modified != last_modified:\n",
    "            # File has changed, open it and see if there are any relevant events in the new information, reset last_modified flag\n",
    "            with open(most_recent_file) as f:\n",
    "                todays_events = make_df_from_high_quote_count(most_recent_file,active_high_quote_threshold)\n",
    "                if len(todays_events) != current_length:\n",
    "                    #There are new events check if the events are related to symbols that have already been published. Get Unique symbols\n",
    "                    msg_text,new_symbols=generate_alert_strings(temp,todays_events)\n",
    "                    # unpublished_symbols = [symbol for symbol in new_symbols if symbol not in symbols]\n",
    "                    positions = np.where(np.isin(new_symbols, symbols, invert=True))\n",
    "                    if len(positions[0]) > 0:\n",
    "                         # positions = [new_symbols.index(symbol) for symbol in unpublished_symbols]\n",
    "                        # selected_messages = [msg_text[position] for position in positions]\n",
    "                        selected_messages = [msg_text[position] for position in np.concatenate(positions)]\n",
    "                        for msg in selected_messages:\n",
    "                            display(Markdown(msg))\n",
    "                            # send_slack_message('#burst_report',msg,slack_api_token)\n",
    "                            # response = send_twitter_message(msg,service_name='KII')\n",
    "                            # response = send_twitter_message(msg,service_name='HFT')\n",
    "                            published_msg.append(msg)\n",
    "                    symbols = new_symbols    \n",
    "                    current_length=len(todays_events)\n",
    "                f.close()\n",
    "        last_modified = os.path.getmtime(most_recent_file)\n",
    "        # total_events = len(todays_events)\n",
    "        # total_events_symbols=todays_events['Symbol'].value_counts()\n",
    "        # # Create the event_list_msg\n",
    "        # # Create the event_list_msg\n",
    "        # event_list_msg = ' '.join([f'${row[0]}-{row[1]}' for row in total_events_symbols.iter_rows()])\n",
    "\n",
    "        # # print(f'Daily Wrap - Tickers: {event_list_msg}')\n",
    "        # final_msg = f'Daily Wrap up for {today} - {total_events} events.'# Threshold - {high_quote_threshold}\n",
    "        # print(f'Daily Wrap - Final Message: {final_msg}')\n",
    "\n",
    "        # user_input = input(\"Type 'exit' to stop: \")\n",
    "        # if user_input.lower() == 'exit':\n",
    "        #     break\n",
    "\n",
    "        sleep(10)\n",
    "\n",
    "\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "        print('Keyboard interrupt detected. Exiting...')\n",
    "        total_events = len(todays_events)\n",
    "        total_events_symbols=todays_events['Symbol'].value_counts()\n",
    "        total_events_symbols =  total_events_symbols.sort(by='count', descending=True)\n",
    "        # Create the event_list_msg\n",
    "        event_list_msg = ' '.join([f'${row[0]}-{row[1]}' for row in total_events_symbols.iter_rows()])\n",
    "\n",
    "        # print(f'Daily Wrap - Tickers: {event_list_msg}')\n",
    "        final_msg = f'Daily Wrap up for {today} - {total_events} events.'# Threshold - {high_quote_threshold}\n",
    "        # message = event_list_msg\n",
    "        # pieces = split_message(message, max_length=279)\n",
    "        print(f'Daily Wrap - Tickers: {event_list_msg}')    \n",
    "\n",
    "        # for i, piece in enumerate(pieces):\n",
    "        # for i, item in reversed(list(enumerate(pieces))):\n",
    "        #     print(f'Daily Wrap - Tickers: {item}')\n",
    "\n",
    "        print(f'Daily Wrap - Final Message: {final_msg}')\n",
    "# #             # send_slack_message('#burst_report',piece,slack_api_token)\n",
    "# #             # response = send_twitter_message(item,service_name='KII')\n",
    "# #             # response = send_twitter_message(item,service_name='HFT')\n",
    "# #         # send_slack_message('#burst_report',final_msg,slack_api_token)\n",
    "# #         # response = send_twitter_message(final_msg,service_name='KII')\n",
    "# #         # response = send_twitter_message(final_msg,service_name='HFT')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
