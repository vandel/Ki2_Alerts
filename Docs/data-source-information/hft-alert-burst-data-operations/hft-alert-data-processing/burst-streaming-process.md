# Burst Streaming Process

The Python code that monitors incoming Burst\_Monitor  data is a script that connects to an MQTT broker, subscribes to a topic, and monitors changes in four specific files. If any of these files change, the script publishes new data to specified MQTT topics. Here's a step-by-step description of what the script does:

1. The script starts by defining `mqtt_message` as a global variable.
2. It then defines a function `connect_mqtt` that connects to an MQTT broker. This function defines two callback functions: `on_connect` and `on_message`. `on_connect` is called when the client connects to the broker and prints a message indicating whether the connection was successful. `on_message` is called when a message is received on a subscribed topic and updates the global variable `mqtt_message` with the received message.
3. The script calls `connect_mqtt` to connect to the MQTT broker and starts the MQTT client loop.
4. It subscribes to the topic "burst\_abort".
5. It gets a list of all files in two directories and filters these lists to include only files that start with the current date followed by '\_hft1.csv', '\_hft2.csv', '\_hft3.csv', or '.csv'.
6. If any of these files are not found, it raises a `ValueError` and prints "All files not available".
7. If all files are found, it enters a loop where it monitors changes in these files. If a file changes, it calls the corresponding function (`monitor_hft1_stream`, `monitor_burst_stream`, `monitor_hft2_stream`, or `monitor_hft3_stream`) to process the new data and publish it to the corresponding MQTT topic.
8. The loop continues until a message 'quit' is received on the subscribed topic, at which point it sets `terminate` to `True`, resets `mqtt_message` to "none", and prints 'MQTT interrupt detected, cancelling the task...'.
9. Finally, it stops the MQTT client loop and disconnects from the MQTT broker.

The description above describes the real-time monitoring of data as it comes in during a trading day.  In order to provide for situations where the data flow is interrupted or this initial step in data processing is not active as well as to support performing this processing step on archival data, as system pf monitoring the progress of evaluating the source .csv file progress is implemented.  This involves individual pickle files that are maintained by the stream processing subroutines that are stored in the Processed\_Data directory along with the NDJSON data itself.  The filename convention for these are YYYYMMDD\_HFTx\_last\_size.pkl where x is 1,2,or 3 depending on which file is being tracked.  If a date is selected for processing and the pickle file for that date is not found the entire file is processed.

The data captured in this process is saved in two NDJSON files in the Processed\_Data directory.  The name of these files is of the form of YYYYMMDD\_processed.json and YYYYMMDD\_processed\_1.json.  The YYYYMMDD\_processed.json file  is generated by the monitor\_hft1\_stream subroutine in Burst\_Stream\_3.py and represents the stream of trade and quote information with NumQuotes greater than 1000 that appears in the left hand panel of the Burst Monitor Program and the  YYYYMMDD\_processed\_1.json file is generated by the monitor\_burst\_stream subroutine and contains the information that is presented in the center panel of the  Burst Monitor Program augmented by the details of the Symbols that are in the Bursts as presented in the right hand panel of the Burst Monitor Program.

